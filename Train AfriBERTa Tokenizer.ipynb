{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIjlm9qgrJuMqJV+KxHhP6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"93AfnqUtevOk"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"code","source":["from datasets import load_dataset\n","'''\n","load corpus file used to train tokenizer of a model. \n","change the path of data_files to path of your corpus saved.\n","you can download a corpus from several resource, such as Sadilar, LeipZig, GitHub, etc.\n","'''\n","raw_datasets = load_dataset('text', data_files=\"/content/drive/MyDrive/akan twi.txt\")"],"metadata":{"id":"8eZfdJM9e-J_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","'''\n","train the old tokenizer of AfriBERTa with new corpus.\n","we follow tutorial of Training a new tokenizer from an old one in HuggingFace course (https://huggingface.co/course/chapter6/2#training-a-new-tokenizer-from-an-old-one)\n","'''\n","\n","old_tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_large\")\n","\n","training_corpus = raw_datasets[\"train\"][\"text\"]\n","tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n","\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/Tokenizer AfriBERTa\") #change with the directory path to save the trained tokenizer"],"metadata":{"id":"CKVBbc5bgLsf"},"execution_count":null,"outputs":[]}]}